{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfa200f-9225-47c6-9ccc-3e39c6f78624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Vector DB already contains 789 documents. Skipping ingestion.\n",
      "To force re-ingestion, delete the 'spark_rag_db' folder.\n",
      "\n",
      "==================================================\n",
      "ü§ñ SPARK DIAGNOSTIC CHATBOT READY\n",
      "==================================================\n",
      "Commands:\n",
      "  - Type your question to search ALL logs.\n",
      "  - Type 'set app <AppID>' to focus on one application.\n",
      "  - Type 'clear app' to return to global search.\n",
      "  - Type 'quit' to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ALL APPS] You:  what are the shortest running applications ? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching across ALL application logs...\n",
      "\n",
      "Gemini:\n",
      "Based on the provided log summaries, it is not possible to determine the shortest running applications, as the logs only provide performance summaries for individual stages within applications, not the overall application runtime.\n",
      "\n",
      "However, an analysis of the provided stage performance summaries reveals a common bottleneck across multiple applications:\n",
      "\n",
      "**Root Cause:**\n",
      "All identified applications are experiencing a **DESERIALIZATION_BOTTLENECK** within various stages. This indicates that a significant portion of the task runtime is spent converting serialized data (e.g., data received during shuffles or read from storage) back into Java objects.\n",
      "\n",
      "**Application-Specific Issues:**\n",
      "\n",
      "*   **app-20211011151946-6418:** Stage 4 exhibits high deserialization time (80.2% of runtime).\n",
      "*   **app-20211011153421-6435:** Stage 0 exhibits high deserialization time (64.7% of runtime).\n",
      "*   **app-20211012224801-11445:** Stage 1 exhibits high deserialization time (17.8% of runtime).\n",
      "*   **app-20211011152915-6429:** Stage 1 exhibits high deserialization time (18.0% of runtime).\n",
      "*   **app-20211012234543-11586:** Stage 0 exhibits high deserialization time (40.6% of runtime).\n",
      "\n",
      "**Explanation of terms not present in logs:**\n",
      "The provided logs do not mention 'Data Skew', 'GC Pressure', or 'Shuffle' as specific issues.\n",
      "\n",
      "**Suggested Configuration Changes to address DESERIALIZATION_BOTTLENECK:**\n",
      "\n",
      "1.  **Switch to Kryo Serialization:**\n",
      "    *   Spark's default Java serialization is verbose and slow. Kryo serialization is generally more compact and faster.\n",
      "    *   **Configuration:**\n",
      "        ```\n",
      "        spark.serializer org.apache.spark.serializer.KryoSerializer\n",
      "        spark.kryoserializer.buffer.max 256m  (or higher if needed for large objects)\n",
      "        ```\n",
      "    *   Register custom classes with Kryo to optimize further:\n",
      "        ```python\n",
      "        # Example in Python\n",
      "        from pyspark import SparkConf\n",
      "        conf = SparkConf().setAppName(\"MyApp\") \\\n",
      "            .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
      "            .set(\"spark.kryo.registrator\", \"com.example.MyKryoRegistrator\")\n",
      "        ```\n",
      "        A custom `MyKryoRegistrator` class (in Scala/Java) would register common classes used in your application.\n",
      "\n",
      "2.  **Optimize Data Formats:**\n",
      "    *   When reading/writing data to storage, use efficient columnar formats like Parquet or ORC, which are optimized for performance and space, reducing deserialization overhead.\n",
      "\n",
      "3.  **Minimize Object Creation and Serialization/Deserialization:**\n",
      "    *   Review the Spark job logic to identify opportunities to reduce the amount of data being shuffled or cached, especially if complex or custom objects are involved.\n",
      "    *   Consider using primitive types or case classes/POJOs instead of generic collections or less efficient custom objects.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ALL APPS] You:  which apps ran the longest ? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching across ALL application logs...\n",
      "\n",
      "Gemini:\n",
      "Based on the provided log summaries, the applications that ran the longest are:\n",
      "\n",
      "*   **app-20211011231944-7649**: Ran for 10.32 seconds.\n",
      "*   **app-20211011174919-6767**: Ran for 9.64 seconds.\n",
      "*   **app-20211012230523-11486**: Ran for 9.64 seconds.\n",
      "\n",
      "All applications completed successfully with no reported issues. Therefore, no root cause analysis or configuration changes are suggested.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ALL APPS] You:  can you compare the longest and shortest running application ? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching across ALL application logs...\n",
      "\n",
      "Gemini:\n",
      "Based on the provided log summaries:\n",
      "\n",
      "The longest running application is **app-20211011231944-7649**, which completed Job 1 in 10.32 seconds.\n",
      "\n",
      "The shortest running applications are **app-20211011151817-6416** and **app-20211011152324-6422**, both of which completed Job 1 in 9.32 seconds.\n",
      "\n",
      "Comparing the longest and shortest runs, there is a difference of approximately 1 second (10.32s vs. 9.32s).\n",
      "\n",
      "**Analysis:**\n",
      "All applications reported a \"JobSucceeded\" status and no issues were identified in their respective `JobPerformanceSummary` logs. The observed variance in execution time is minimal (approximately 1 second difference between the longest and shortest runs) and does not indicate any performance degradation or specific root causes such as Data Skew, GC Pressure, or Shuffle issues based solely on these summaries.\n",
      "\n",
      "**Recommendations:**\n",
      "Given that all jobs succeeded and the performance variance is small, no immediate configuration changes are suggested. Further detailed logs would be required to analyze minor performance differences if these become a concern in a larger fleet.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 330\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# Show current context in the prompt\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     prompt_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_context_app\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] You: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_context_app \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ALL APPS] You: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 330\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(prompt_prefix)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# Exit\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from statistics import median\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION\n",
    "# ==========================================\n",
    "# Export this in your terminal: export GOOGLE_API_KEY=\"your_key\"\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") \n",
    "DB_PATH = \"./spark_rag_db\" # Where vectors will be saved on disk\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"‚ö†Ô∏è  WARNING: GOOGLE_API_KEY environment variable not set.\")\n",
    "else:\n",
    "    genai.configure(api_key=API_KEY)\n",
    "\n",
    "# ==========================================\n",
    "# 1. PARSING LOGIC (Your Code)\n",
    "# ==========================================\n",
    "\n",
    "class SparkLogAnalysis:\n",
    "    \"\"\"A container object for all parsed summaries from a Spark event log.\"\"\"\n",
    "    def __init__(self, log_file_path):\n",
    "        self.application_id = \"unknown_app\"\n",
    "        self.log_file = os.path.basename(log_file_path)\n",
    "        self.job_summaries = []\n",
    "        self.stage_summaries = []\n",
    "        self.application_summaries = {}\n",
    "\n",
    "    def set_application_id(self, app_id):\n",
    "        if app_id:\n",
    "            self.application_id = app_id\n",
    "\n",
    "    def add_summary(self, summary_dict):\n",
    "        if not summary_dict: return\n",
    "        event_type = summary_dict.get(\"event_type\")\n",
    "        summary_dict[\"application_id\"] = self.application_id\n",
    "\n",
    "        if event_type == \"JobPerformanceSummary\":\n",
    "            self.job_summaries.append(summary_dict)\n",
    "        elif event_type == \"StagePerformanceSummary\":\n",
    "            self.stage_summaries.append(summary_dict)\n",
    "        else:\n",
    "            self.application_summaries[event_type] = summary_dict\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"application_id\": self.application_id,\n",
    "            \"log_file\": self.log_file,\n",
    "            \"job_summaries\": self.job_summaries,\n",
    "            \"stage_summaries\": self.stage_summaries,\n",
    "            \"application_summaries\": self.application_summaries\n",
    "        }\n",
    "\n",
    "def analyze_spark_log(log_file_path):\n",
    "    \"\"\"Parses a single Spark event log.\"\"\"\n",
    "    analysis_object = SparkLogAnalysis(log_file_path)\n",
    "    stage_data = {}\n",
    "    job_data = {}\n",
    "    executor_lifecycle = {}\n",
    "    disk_related_executor_failures = {}\n",
    "    blacklisted_executors = []\n",
    "\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    event = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                event_type = event.get(\"Event\")\n",
    "                \n",
    "                if event_type == \"SparkListenerApplicationStart\":\n",
    "                    analysis_object.set_application_id(event.get(\"App ID\"))\n",
    "                \n",
    "                elif event_type == \"SparkListenerExecutorAdded\":\n",
    "                    executor_id, timestamp = event.get(\"Executor ID\"), event.get(\"Timestamp\")\n",
    "                    executor_lifecycle.setdefault(executor_id, {})['add_time'] = timestamp\n",
    "\n",
    "                elif event_type == \"SparkListenerExecutorRemoved\":\n",
    "                    executor_id, timestamp = event.get(\"Executor ID\"), event.get(\"Timestamp\")\n",
    "                    reason = event.get(\"Removed Reason\", \"\")\n",
    "                    executor_lifecycle.setdefault(executor_id, {})['remove_time'] = timestamp\n",
    "                    if \"no space left on device\" in reason.lower() or \"disk\" in reason.lower():\n",
    "                        disk_related_executor_failures.setdefault(reason, {\"count\": 0, \"samples\": []})\n",
    "                        disk_related_executor_failures[reason][\"count\"] += 1\n",
    "                        if len(disk_related_executor_failures[reason][\"samples\"]) < 3:\n",
    "                            disk_related_executor_failures[reason][\"samples\"].append({\"executor_id\": executor_id, \"timestamp\": timestamp})\n",
    "\n",
    "                elif event_type == \"SparkListenerExecutorBlacklisted\":\n",
    "                    blacklisted_executors.append({\"executor_id\": event.get(\"Executor ID\"), \"timestamp\": event.get(\"Timestamp\"), \"reason\": f\"Blacklisted for {event.get('Task ID')} task failures\"})\n",
    "\n",
    "                elif event_type == \"SparkListenerJobStart\":\n",
    "                    job_id = event.get(\"Job ID\")\n",
    "                    job_data[job_id] = {\"start_time\": event.get(\"Submission Time\")}\n",
    "\n",
    "                elif event_type == \"SparkListenerJobEnd\":\n",
    "                    job_id = event.get(\"Job ID\")\n",
    "                    if job_id in job_data:\n",
    "                        duration_ms = event.get(\"Completion Time\") - job_data[job_id][\"start_time\"]\n",
    "                        job_result = event.get(\"Job Result\", {}).get(\"Result\")\n",
    "                        analysis_object.add_summary({ \"event_type\": \"JobPerformanceSummary\", \"job_id\": job_id, \"status\": job_result, \"duration_s\": duration_ms / 1000, \"summary\": f\"Job {job_id} finished with status {job_result} in {duration_ms / 1000:.2f} seconds.\" })\n",
    "                        del job_data[job_id]\n",
    "\n",
    "                elif event_type == \"SparkListenerTaskEnd\":\n",
    "                    stage_id = event.get(\"Stage ID\")\n",
    "                    if stage_id is not None:\n",
    "                        stage_data.setdefault(stage_id, {\"tasks\": [], \"failed_task_count\": 0})\n",
    "                        if event.get(\"Task End Reason\", {}).get(\"Reason\") != \"Success\": stage_data[stage_id][\"failed_task_count\"] += 1\n",
    "                        task_info, task_metrics = event.get(\"Task Info\", {}), event.get(\"Task Metrics\", {})\n",
    "                        duration_ms = task_info.get(\"Finish Time\", 0) - task_info.get(\"Launch Time\", 0)\n",
    "                        stage_data[stage_id][\"tasks\"].append({ \"duration_ms\": duration_ms, \"jvm_gc_time_ms\": task_metrics.get(\"JVM GC Time\", 0), \"executor_run_time_ms\": task_metrics.get(\"Executor Run Time\", 1), \"executor_deserialize_time_ms\": task_metrics.get(\"Executor Deserialize Time\", 0), \"shuffle_fetch_wait_time_ms\": task_metrics.get(\"Shuffle Read Metrics\", {}).get(\"Fetch Wait Time\", 0), \"memory_spilled_bytes\": task_metrics.get(\"Memory Bytes Spilled\", 0), \"disk_spilled_bytes\": task_metrics.get(\"Disk Bytes Spilled\", 0), })\n",
    "\n",
    "                elif event_type == \"SparkListenerStageCompleted\":\n",
    "                    stage_id = event.get(\"Stage Info\", {}).get(\"Stage ID\")\n",
    "                    if stage_id in stage_data and len(stage_data[stage_id][\"tasks\"]) > 0:\n",
    "                        tasks, failed_task_count = stage_data[stage_id][\"tasks\"], stage_data[stage_id][\"failed_task_count\"]\n",
    "                        task_durations = [t[\"duration_ms\"] for t in tasks]\n",
    "                        total_duration, total_gc_time, total_runtime = sum(task_durations), sum(t[\"jvm_gc_time_ms\"] for t in tasks), sum(t[\"executor_run_time_ms\"] for t in tasks)\n",
    "                        total_deserialize_time, total_fetch_wait_time = sum(t[\"executor_deserialize_time_ms\"] for t in tasks), sum(t[\"shuffle_fetch_wait_time_ms\"] for t in tasks)\n",
    "                        total_spilled_bytes = sum(t[\"memory_spilled_bytes\"] for t in tasks) + sum(t[\"disk_spilled_bytes\"] for t in tasks)\n",
    "                        summary_parts, potential_issues = [f\"Stage {stage_id} completed with {len(tasks)} tasks.\"], []\n",
    "                        if failed_task_count > 0: summary_parts.append(f\"It experienced {failed_task_count} task failures that required retries.\"); potential_issues.append(\"TASK_FAILURES\")\n",
    "                        max_duration, median_duration = max(task_durations), median(task_durations)\n",
    "                        if max_duration > 3 * median_duration and max_duration > 20000: potential_issues.append(\"DATA_SKEW\"); summary_parts.append(f\"Detected potential data skew. Max task duration: {max_duration/1000:.2f}s, median: {median_duration/1000:.2f}s.\")\n",
    "                        if total_runtime > 0 and (total_gc_time / total_runtime) > 0.10: potential_issues.append(\"HIGH_GC_PRESSURE\"); summary_parts.append(f\"High JVM GC pressure detected ({ (total_gc_time / total_runtime) * 100:.1f}% of runtime).\")\n",
    "                        if total_spilled_bytes > 0: potential_issues.append(\"DATA_SPILL\"); summary_parts.append(f\"Detected data spilling to disk ({total_spilled_bytes / (1024*1024):.2f} MB).\")\n",
    "                        if total_duration > 0 and (total_fetch_wait_time / total_duration) > 0.25: potential_issues.append(\"SHUFFLE_BOTTLENECK\"); summary_parts.append(f\"Significant shuffle bottleneck, tasks spent {(total_fetch_wait_time / total_duration) * 100:.1f}% of time waiting for data.\")\n",
    "                        if total_runtime > 0 and (total_deserialize_time / total_runtime) > 0.15: potential_issues.append(\"DESERIALIZATION_BOTTLENECK\"); summary_parts.append(f\"High deserialization time ({(total_deserialize_time / total_runtime) * 100:.1f}% of runtime).\")\n",
    "                        analysis_object.add_summary({ \"event_type\": \"StagePerformanceSummary\", \"stage_id\": stage_id, \"metrics\": { \"task_count\": len(tasks), \"failed_task_count\": failed_task_count, \"max_task_duration_ms\": max_duration, \"median_task_duration_ms\": median_duration, \"total_spilled_mb\": total_spilled_bytes / (1024*1024) }, \"summary\": \" \".join(summary_parts), \"potential_issues\": potential_issues if potential_issues else [\"NONE\"] })\n",
    "\n",
    "        if disk_related_executor_failures:\n",
    "            total_failures = sum(v['count'] for v in disk_related_executor_failures.values())\n",
    "            analysis_object.add_summary({ \"event_type\": \"ExecutorDiskFailureSummary\", \"total_failures\": total_failures, \"summary\": f\"A critical error occurred where {total_failures} executors failed to launch due to running out of disk space on the nodes.\", \"failures_by_reason\": disk_related_executor_failures })\n",
    "        \n",
    "        short_lived_executors = []\n",
    "        churn_threshold_ms = 60000 \n",
    "        for exec_id, times in executor_lifecycle.items():\n",
    "            if 'add_time' in times and 'remove_time' in times and (times['remove_time'] - times['add_time']) < churn_threshold_ms:\n",
    "                short_lived_executors.append({\"executor_id\": exec_id, \"lifespan_s\": (times['remove_time'] - times['add_time']) / 1000})\n",
    "        if short_lived_executors:\n",
    "            avg_lifespan_s = sum(e['lifespan_s'] for e in short_lived_executors) / len(short_lived_executors)\n",
    "            analysis_object.add_summary({ \"event_type\": \"ExecutorChurnSummary\", \"total_churn_events\": len(short_lived_executors), \"summary\": f\"Detected {len(short_lived_executors)} executors with an average lifespan of just {avg_lifespan_s:.2f}s, indicating severe instability.\", \"average_lifespan_s\": round(avg_lifespan_s, 2), \"sample_churned_executors\": short_lived_executors[:3] })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    return analysis_object\n",
    "\n",
    "# ==========================================\n",
    "# 2. VECTORIZATION (ChromaDB Integration)\n",
    "# ==========================================\n",
    "\n",
    "def get_chroma_collection():\n",
    "    \"\"\"Returns a persistent ChromaDB collection.\"\"\"\n",
    "    # Using PersistentClient to save data to disk so we don't re-index every run\n",
    "    client = chromadb.PersistentClient(path=DB_PATH)\n",
    "    # Using the default embedding model (all-MiniLM-L6-v2)\n",
    "    return client.get_or_create_collection(name=\"spark_logs\")\n",
    "\n",
    "def ingest_logs(log_directory):\n",
    "    \"\"\"Parses logs and ingests them into ChromaDB.\"\"\"\n",
    "    collection = get_chroma_collection()\n",
    "    \n",
    "    # Check if we already have data (optional optimization)\n",
    "    if collection.count() > 0:\n",
    "        print(f\"üìö Vector DB already contains {collection.count()} documents. Skipping ingestion.\")\n",
    "        print(\"To force re-ingestion, delete the 'spark_rag_db' folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Processing logs in: {log_directory}\")\n",
    "    all_analyses = []\n",
    "\n",
    "    for filename in os.listdir(log_directory):\n",
    "        full_path = os.path.join(log_directory, filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            print(f\"   - Parsing {filename}...\")\n",
    "            analysis = analyze_spark_log(full_path)\n",
    "            if analysis:\n",
    "                all_analyses.append(analysis)\n",
    "\n",
    "    print(\"üß© Vectorizing and storing data...\")\n",
    "    documents, metadatas, ids = [], [], []\n",
    "\n",
    "    for analysis in all_analyses:\n",
    "        # Flatten the object\n",
    "        summaries = (analysis.job_summaries + \n",
    "                     analysis.stage_summaries + \n",
    "                     list(analysis.application_summaries.values()))\n",
    "\n",
    "        for i, summary in enumerate(summaries):\n",
    "            # 1. Text to Embed\n",
    "            documents.append(summary[\"summary\"])\n",
    "            \n",
    "            # 2. Metadata Cleaning (Chroma requires simple types)\n",
    "            meta = summary.copy()\n",
    "            del meta[\"summary\"]\n",
    "            \n",
    "            # Convert list/dict fields to strings for metadata storage\n",
    "            for key, val in meta.items():\n",
    "                if isinstance(val, (list, dict)):\n",
    "                    meta[key] = str(val)\n",
    "            \n",
    "            metadatas.append(meta)\n",
    "            ids.append(f\"{analysis.application_id}_{i}\")\n",
    "\n",
    "    if documents:\n",
    "        collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "        print(f\"‚úÖ Successfully ingested {len(documents)} log events into ChromaDB.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid log events found to ingest.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. LLM ORCHESTRATOR (The RAG Logic) - UPDATED\n",
    "# ==========================================\n",
    "\n",
    "def get_gemini_response(user_query, collection, app_id=None):\n",
    "    \"\"\"\n",
    "    1. Retrieval: Search ChromaDB. If app_id is provided, filter by it.\n",
    "       Otherwise, search across ALL applications.\n",
    "    2. Augmentation: Build a prompt with the search results.\n",
    "    3. Generation: Call Gemini API.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. RETRIEVAL\n",
    "    query_args = {\n",
    "        \"query_texts\": [user_query],\n",
    "        \"n_results\": 5\n",
    "    }\n",
    "\n",
    "    # Only apply the filter if an app_id is specifically requested\n",
    "    if app_id:\n",
    "        print(f\"üîç Searching logs specifically for App: {app_id}...\")\n",
    "        query_args[\"where\"] = {\"application_id\": app_id}\n",
    "    else:\n",
    "        print(f\"üîç Searching across ALL application logs...\")\n",
    "\n",
    "    results = collection.query(**query_args)\n",
    "\n",
    "    retrieved_docs = results['documents'][0]\n",
    "    retrieved_meta = results['metadatas'][0]\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"I couldn't find any log events matching your question.\"\n",
    "\n",
    "    # 2. AUGMENTATION (Context Construction)\n",
    "    context_parts = []\n",
    "    for doc, meta in zip(retrieved_docs, retrieved_meta):\n",
    "        # We add the App ID to the context so the LLM knows which app the log belongs to\n",
    "        # when searching globally.\n",
    "        current_app = meta.get('application_id', 'unknown')\n",
    "        info_tag = f\"[App: {current_app} | {meta.get('event_type', 'Event')}]\"\n",
    "        issue_tag = f\"[Issues: {meta.get('potential_issues', 'None')}]\"\n",
    "        context_parts.append(f\"{info_tag} {issue_tag}\\nLog Summary: {doc}\")\n",
    "\n",
    "    context_str = \"\\n---\\n\".join(context_parts)\n",
    "\n",
    "    # Dynamic System Prompt\n",
    "    if app_id:\n",
    "        context_intro = f\"You are analyzing performance logs for a SPECIFIC Spark Application: {app_id}.\"\n",
    "    else:\n",
    "        context_intro = \"You are analyzing a fleet of Spark Applications. The logs provided may belong to different applications.\"\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert Apache Spark Log Diagnostician. \n",
    "    {context_intro}\n",
    "\n",
    "    Here are the most relevant log summaries retrieved from the system:\n",
    "    \n",
    "    {context_str}\n",
    "\n",
    "    USER QUESTION: \"{user_query}\"\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Identify root causes based ONLY on the provided log summaries.\n",
    "    2. If analyzing multiple apps, clearly state which Application ID had which issue.\n",
    "    3. If the logs mention 'Data Skew', 'GC Pressure', or 'Shuffle', explain what that means.\n",
    "    4. Suggest concrete configuration changes where applicable.\n",
    "    5. Keep the tone professional and concise.\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. GENERATION\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        response = model.generate_content(system_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error communicating with Gemini API: {e}\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION - UPDATED\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # A. Setup paths\n",
    "    LOG_DIR = r\"C:\\Users\\ddev\\Documents\\projects\\spark-event-logs\\test-logs\"\n",
    "    \n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        print(f\"‚ùå Error: Log directory '{LOG_DIR}' does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    # B. Ingest Logs\n",
    "    ingest_logs(LOG_DIR)\n",
    "    \n",
    "    # C. Get the collection\n",
    "    collection = get_chroma_collection()\n",
    "    \n",
    "    # D. Interactive Chat Loop\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ü§ñ SPARK DIAGNOSTIC CHATBOT READY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"Commands:\")\n",
    "    print(\"  - Type your question to search ALL logs.\")\n",
    "    print(\"  - Type 'set app <AppID>' to focus on one application.\")\n",
    "    print(\"  - Type 'clear app' to return to global search.\")\n",
    "    print(\"  - Type 'quit' to exit.\\n\")\n",
    "\n",
    "    current_context_app = None\n",
    "\n",
    "    while True:\n",
    "        # Show current context in the prompt\n",
    "        prompt_prefix = f\"[{current_context_app}] You: \" if current_context_app else \"[ALL APPS] You: \"\n",
    "        user_input = input(prompt_prefix).strip()\n",
    "\n",
    "        # Exit\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            break\n",
    "        \n",
    "        # Command: Set Context\n",
    "        if user_input.lower().startswith(\"set app \"):\n",
    "            parts = user_input.split(\" \")\n",
    "            if len(parts) > 2:\n",
    "                current_context_app = parts[2].strip()\n",
    "                print(f\"üîí Context locked to: {current_context_app}\")\n",
    "            continue\n",
    "\n",
    "        # Command: Clear Context\n",
    "        if user_input.lower() == \"clear app\":\n",
    "            current_context_app = None\n",
    "            print(f\"üîì Context cleared. Searching all logs.\")\n",
    "            continue\n",
    "        \n",
    "        # Standard Query\n",
    "        answer = get_gemini_response(user_input, collection, app_id=current_context_app)\n",
    "        print(f\"\\nGemini:\\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba91adf-6784-4d58-8329-9abbdb852cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\ddev\\anaconda3\\lib\\site-packages (5.29.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (1.10.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.35.3)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (2.11.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ddev\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7551fc-d58a-486f-ab1e-9dc3c57bea64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
