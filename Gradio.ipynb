{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fdcf1b-e466-48a1-9488-71af9274aee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from statistics import median\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION\n",
    "# ==========================================\n",
    "# Export this in your terminal: export GOOGLE_API_KEY=\"your_key\"\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") \n",
    "DB_PATH = \"./spark_rag_db\" # Where vectors will be saved on disk\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"‚ö†Ô∏è  WARNING: GOOGLE_API_KEY environment variable not set.\")\n",
    "else:\n",
    "    genai.configure(api_key=API_KEY)\n",
    "\n",
    "# ==========================================\n",
    "# 1. PARSING LOGIC (Your Code)\n",
    "# ==========================================\n",
    "\n",
    "class SparkLogAnalysis:\n",
    "    \"\"\"A container object for all parsed summaries from a Spark event log.\"\"\"\n",
    "    def __init__(self, log_file_path):\n",
    "        self.application_id = \"unknown_app\"\n",
    "        self.log_file = os.path.basename(log_file_path)\n",
    "        self.job_summaries = []\n",
    "        self.stage_summaries = []\n",
    "        self.application_summaries = {}\n",
    "\n",
    "    def set_application_id(self, app_id):\n",
    "        if app_id:\n",
    "            self.application_id = app_id\n",
    "\n",
    "    def add_summary(self, summary_dict):\n",
    "        if not summary_dict: return\n",
    "        event_type = summary_dict.get(\"event_type\")\n",
    "        summary_dict[\"application_id\"] = self.application_id\n",
    "\n",
    "        if event_type == \"JobPerformanceSummary\":\n",
    "            self.job_summaries.append(summary_dict)\n",
    "        elif event_type == \"StagePerformanceSummary\":\n",
    "            self.stage_summaries.append(summary_dict)\n",
    "        else:\n",
    "            self.application_summaries[event_type] = summary_dict\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"application_id\": self.application_id,\n",
    "            \"log_file\": self.log_file,\n",
    "            \"job_summaries\": self.job_summaries,\n",
    "            \"stage_summaries\": self.stage_summaries,\n",
    "            \"application_summaries\": self.application_summaries\n",
    "        }\n",
    "\n",
    "def analyze_spark_log(log_file_path):\n",
    "    \"\"\"Parses a single Spark event log.\"\"\"\n",
    "    analysis_object = SparkLogAnalysis(log_file_path)\n",
    "    stage_data = {}\n",
    "    job_data = {}\n",
    "    executor_lifecycle = {}\n",
    "    disk_related_executor_failures = {}\n",
    "    blacklisted_executors = []\n",
    "\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    event = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                event_type = event.get(\"Event\")\n",
    "                \n",
    "                if event_type == \"SparkListenerApplicationStart\":\n",
    "                    analysis_object.set_application_id(event.get(\"App ID\"))\n",
    "                \n",
    "                elif event_type == \"SparkListenerExecutorAdded\":\n",
    "                    executor_id, timestamp = event.get(\"Executor ID\"), event.get(\"Timestamp\")\n",
    "                    executor_lifecycle.setdefault(executor_id, {})['add_time'] = timestamp\n",
    "\n",
    "                elif event_type == \"SparkListenerExecutorRemoved\":\n",
    "                    executor_id, timestamp = event.get(\"Executor ID\"), event.get(\"Timestamp\")\n",
    "                    reason = event.get(\"Removed Reason\", \"\")\n",
    "                    executor_lifecycle.setdefault(executor_id, {})['remove_time'] = timestamp\n",
    "                    if \"no space left on device\" in reason.lower() or \"disk\" in reason.lower():\n",
    "                        disk_related_executor_failures.setdefault(reason, {\"count\": 0, \"samples\": []})\n",
    "                        disk_related_executor_failures[reason][\"count\"] += 1\n",
    "                        if len(disk_related_executor_failures[reason][\"samples\"]) < 3:\n",
    "                            disk_related_executor_failures[reason][\"samples\"].append({\"executor_id\": executor_id, \"timestamp\": timestamp})\n",
    "\n",
    "                elif event_type == \"SparkListenerExecutorBlacklisted\":\n",
    "                    blacklisted_executors.append({\"executor_id\": event.get(\"Executor ID\"), \"timestamp\": event.get(\"Timestamp\"), \"reason\": f\"Blacklisted for {event.get('Task ID')} task failures\"})\n",
    "\n",
    "                elif event_type == \"SparkListenerJobStart\":\n",
    "                    job_id = event.get(\"Job ID\")\n",
    "                    job_data[job_id] = {\"start_time\": event.get(\"Submission Time\")}\n",
    "\n",
    "                elif event_type == \"SparkListenerJobEnd\":\n",
    "                    job_id = event.get(\"Job ID\")\n",
    "                    if job_id in job_data:\n",
    "                        duration_ms = event.get(\"Completion Time\") - job_data[job_id][\"start_time\"]\n",
    "                        job_result = event.get(\"Job Result\", {}).get(\"Result\")\n",
    "                        analysis_object.add_summary({ \"event_type\": \"JobPerformanceSummary\", \"job_id\": job_id, \"status\": job_result, \"duration_s\": duration_ms / 1000, \"summary\": f\"Job {job_id} finished with status {job_result} in {duration_ms / 1000:.2f} seconds.\" })\n",
    "                        del job_data[job_id]\n",
    "\n",
    "                elif event_type == \"SparkListenerTaskEnd\":\n",
    "                    stage_id = event.get(\"Stage ID\")\n",
    "                    if stage_id is not None:\n",
    "                        stage_data.setdefault(stage_id, {\"tasks\": [], \"failed_task_count\": 0})\n",
    "                        if event.get(\"Task End Reason\", {}).get(\"Reason\") != \"Success\": stage_data[stage_id][\"failed_task_count\"] += 1\n",
    "                        task_info, task_metrics = event.get(\"Task Info\", {}), event.get(\"Task Metrics\", {})\n",
    "                        duration_ms = task_info.get(\"Finish Time\", 0) - task_info.get(\"Launch Time\", 0)\n",
    "                        stage_data[stage_id][\"tasks\"].append({ \"duration_ms\": duration_ms, \"jvm_gc_time_ms\": task_metrics.get(\"JVM GC Time\", 0), \"executor_run_time_ms\": task_metrics.get(\"Executor Run Time\", 1), \"executor_deserialize_time_ms\": task_metrics.get(\"Executor Deserialize Time\", 0), \"shuffle_fetch_wait_time_ms\": task_metrics.get(\"Shuffle Read Metrics\", {}).get(\"Fetch Wait Time\", 0), \"memory_spilled_bytes\": task_metrics.get(\"Memory Bytes Spilled\", 0), \"disk_spilled_bytes\": task_metrics.get(\"Disk Bytes Spilled\", 0), })\n",
    "\n",
    "                elif event_type == \"SparkListenerStageCompleted\":\n",
    "                    stage_id = event.get(\"Stage Info\", {}).get(\"Stage ID\")\n",
    "                    if stage_id in stage_data and len(stage_data[stage_id][\"tasks\"]) > 0:\n",
    "                        tasks, failed_task_count = stage_data[stage_id][\"tasks\"], stage_data[stage_id][\"failed_task_count\"]\n",
    "                        task_durations = [t[\"duration_ms\"] for t in tasks]\n",
    "                        total_duration, total_gc_time, total_runtime = sum(task_durations), sum(t[\"jvm_gc_time_ms\"] for t in tasks), sum(t[\"executor_run_time_ms\"] for t in tasks)\n",
    "                        total_deserialize_time, total_fetch_wait_time = sum(t[\"executor_deserialize_time_ms\"] for t in tasks), sum(t[\"shuffle_fetch_wait_time_ms\"] for t in tasks)\n",
    "                        total_spilled_bytes = sum(t[\"memory_spilled_bytes\"] for t in tasks) + sum(t[\"disk_spilled_bytes\"] for t in tasks)\n",
    "                        summary_parts, potential_issues = [f\"Stage {stage_id} completed with {len(tasks)} tasks.\"], []\n",
    "                        if failed_task_count > 0: summary_parts.append(f\"It experienced {failed_task_count} task failures that required retries.\"); potential_issues.append(\"TASK_FAILURES\")\n",
    "                        max_duration, median_duration = max(task_durations), median(task_durations)\n",
    "                        if max_duration > 3 * median_duration and max_duration > 20000: potential_issues.append(\"DATA_SKEW\"); summary_parts.append(f\"Detected potential data skew. Max task duration: {max_duration/1000:.2f}s, median: {median_duration/1000:.2f}s.\")\n",
    "                        if total_runtime > 0 and (total_gc_time / total_runtime) > 0.10: potential_issues.append(\"HIGH_GC_PRESSURE\"); summary_parts.append(f\"High JVM GC pressure detected ({ (total_gc_time / total_runtime) * 100:.1f}% of runtime).\")\n",
    "                        if total_spilled_bytes > 0: potential_issues.append(\"DATA_SPILL\"); summary_parts.append(f\"Detected data spilling to disk ({total_spilled_bytes / (1024*1024):.2f} MB).\")\n",
    "                        if total_duration > 0 and (total_fetch_wait_time / total_duration) > 0.25: potential_issues.append(\"SHUFFLE_BOTTLENECK\"); summary_parts.append(f\"Significant shuffle bottleneck, tasks spent {(total_fetch_wait_time / total_duration) * 100:.1f}% of time waiting for data.\")\n",
    "                        if total_runtime > 0 and (total_deserialize_time / total_runtime) > 0.15: potential_issues.append(\"DESERIALIZATION_BOTTLENECK\"); summary_parts.append(f\"High deserialization time ({(total_deserialize_time / total_runtime) * 100:.1f}% of runtime).\")\n",
    "                        analysis_object.add_summary({ \"event_type\": \"StagePerformanceSummary\", \"stage_id\": stage_id, \"metrics\": { \"task_count\": len(tasks), \"failed_task_count\": failed_task_count, \"max_task_duration_ms\": max_duration, \"median_task_duration_ms\": median_duration, \"total_spilled_mb\": total_spilled_bytes / (1024*1024) }, \"summary\": \" \".join(summary_parts), \"potential_issues\": potential_issues if potential_issues else [\"NONE\"] })\n",
    "\n",
    "        if disk_related_executor_failures:\n",
    "            total_failures = sum(v['count'] for v in disk_related_executor_failures.values())\n",
    "            analysis_object.add_summary({ \"event_type\": \"ExecutorDiskFailureSummary\", \"total_failures\": total_failures, \"summary\": f\"A critical error occurred where {total_failures} executors failed to launch due to running out of disk space on the nodes.\", \"failures_by_reason\": disk_related_executor_failures })\n",
    "        \n",
    "        short_lived_executors = []\n",
    "        churn_threshold_ms = 60000 \n",
    "        for exec_id, times in executor_lifecycle.items():\n",
    "            if 'add_time' in times and 'remove_time' in times and (times['remove_time'] - times['add_time']) < churn_threshold_ms:\n",
    "                short_lived_executors.append({\"executor_id\": exec_id, \"lifespan_s\": (times['remove_time'] - times['add_time']) / 1000})\n",
    "        if short_lived_executors:\n",
    "            avg_lifespan_s = sum(e['lifespan_s'] for e in short_lived_executors) / len(short_lived_executors)\n",
    "            analysis_object.add_summary({ \"event_type\": \"ExecutorChurnSummary\", \"total_churn_events\": len(short_lived_executors), \"summary\": f\"Detected {len(short_lived_executors)} executors with an average lifespan of just {avg_lifespan_s:.2f}s, indicating severe instability.\", \"average_lifespan_s\": round(avg_lifespan_s, 2), \"sample_churned_executors\": short_lived_executors[:3] })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    return analysis_object\n",
    "\n",
    "# ==========================================\n",
    "# 2. VECTORIZATION (ChromaDB Integration)\n",
    "# ==========================================\n",
    "\n",
    "def get_chroma_collection():\n",
    "    \"\"\"Returns a persistent ChromaDB collection.\"\"\"\n",
    "    # Using PersistentClient to save data to disk so we don't re-index every run\n",
    "    client = chromadb.PersistentClient(path=DB_PATH)\n",
    "    # Using the default embedding model (all-MiniLM-L6-v2)\n",
    "    return client.get_or_create_collection(name=\"spark_logs\")\n",
    "\n",
    "def ingest_logs(log_directory):\n",
    "    \"\"\"Parses logs and ingests them into ChromaDB.\"\"\"\n",
    "    collection = get_chroma_collection()\n",
    "    \n",
    "    # Check if we already have data (optional optimization)\n",
    "    if collection.count() > 0:\n",
    "        print(f\"üìö Vector DB already contains {collection.count()} documents. Skipping ingestion.\")\n",
    "        print(\"To force re-ingestion, delete the 'spark_rag_db' folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Processing logs in: {log_directory}\")\n",
    "    all_analyses = []\n",
    "\n",
    "    for filename in os.listdir(log_directory):\n",
    "        full_path = os.path.join(log_directory, filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            print(f\"   - Parsing {filename}...\")\n",
    "            analysis = analyze_spark_log(full_path)\n",
    "            if analysis:\n",
    "                all_analyses.append(analysis)\n",
    "\n",
    "    print(\"üß© Vectorizing and storing data...\")\n",
    "    documents, metadatas, ids = [], [], []\n",
    "\n",
    "    for analysis in all_analyses:\n",
    "        # Flatten the object\n",
    "        summaries = (analysis.job_summaries + \n",
    "                     analysis.stage_summaries + \n",
    "                     list(analysis.application_summaries.values()))\n",
    "\n",
    "        for i, summary in enumerate(summaries):\n",
    "            # 1. Text to Embed\n",
    "            documents.append(summary[\"summary\"])\n",
    "            \n",
    "            # 2. Metadata Cleaning (Chroma requires simple types)\n",
    "            meta = summary.copy()\n",
    "            del meta[\"summary\"]\n",
    "            \n",
    "            # Convert list/dict fields to strings for metadata storage\n",
    "            for key, val in meta.items():\n",
    "                if isinstance(val, (list, dict)):\n",
    "                    meta[key] = str(val)\n",
    "            \n",
    "            metadatas.append(meta)\n",
    "            ids.append(f\"{analysis.application_id}_{i}\")\n",
    "\n",
    "    if documents:\n",
    "        collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "        print(f\"‚úÖ Successfully ingested {len(documents)} log events into ChromaDB.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid log events found to ingest.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. LLM ORCHESTRATOR (The RAG Logic) - UPDATED\n",
    "# ==========================================\n",
    "\n",
    "def get_gemini_response(user_query, collection, app_id=None):\n",
    "    \"\"\"\n",
    "    Generator function that streams the response from Gemini.\n",
    "    \"\"\"\n",
    "    # 1. RETRIEVAL (Same as before)\n",
    "    query_args = {\n",
    "        \"query_texts\": [user_query],\n",
    "        \"n_results\": 5\n",
    "    }\n",
    "    \n",
    "    if app_id:\n",
    "        print(f\"üîç Searching logs specifically for App: {app_id}...\")\n",
    "        query_args[\"where\"] = {\"application_id\": app_id}\n",
    "    else:\n",
    "        print(f\"üîç Searching across ALL application logs...\")\n",
    "\n",
    "    results = collection.query(**query_args)\n",
    "    retrieved_docs = results['documents'][0]\n",
    "    retrieved_meta = results['metadatas'][0]\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        yield \"I couldn't find any log events matching your question.\"\n",
    "        return\n",
    "\n",
    "    # 2. AUGMENTATION (Same as before)\n",
    "    context_parts = []\n",
    "    for doc, meta in zip(retrieved_docs, retrieved_meta):\n",
    "        current_app = meta.get('application_id', 'unknown')\n",
    "        info_tag = f\"[App: {current_app} | {meta.get('event_type', 'Event')}]\"\n",
    "        issue_tag = f\"[Issues: {meta.get('potential_issues', 'None')}]\"\n",
    "        context_parts.append(f\"{info_tag} {issue_tag}\\nLog Summary: {doc}\")\n",
    "\n",
    "    context_str = \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    if app_id:\n",
    "        context_intro = f\"You are analyzing performance logs for a SPECIFIC Spark Application: {app_id}.\"\n",
    "    else:\n",
    "        context_intro = \"You are analyzing a fleet of Spark Applications. The logs provided may belong to different applications.\"\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert Apache Spark Log Diagnostician. \n",
    "    {context_intro}\n",
    "\n",
    "    Here are the most relevant log summaries retrieved from the system:\n",
    "    \n",
    "    {context_str}\n",
    "\n",
    "    USER QUESTION: \"{user_query}\"\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Identify root causes based ONLY on the provided log summaries.\n",
    "    2. If analyzing multiple apps, clearly state which Application ID had which issue.\n",
    "    3. If the logs mention 'Data Skew', 'GC Pressure', or 'Shuffle', explain what that means.\n",
    "    4. Suggest concrete configuration changes where applicable.\n",
    "    5. Keep the tone professional and concise.\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. GENERATION (Streaming Mode)\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        # Enable streaming\n",
    "        response_stream = model.generate_content(system_prompt, stream=True)\n",
    "        \n",
    "        partial_text = \"\"\n",
    "        for chunk in response_stream:\n",
    "            if chunk.text:\n",
    "                partial_text += chunk.text\n",
    "                yield partial_text  # Yield the updated full text at every step\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"‚ùå Error communicating with Gemini API: {e}\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. GRADIO UI IMPLEMENTATION\n",
    "# ==========================================\n",
    "\n",
    "def get_all_app_ids():\n",
    "    \"\"\"Helper to fetch unique Application IDs from ChromaDB for the dropdown.\"\"\"\n",
    "    try:\n",
    "        collection = get_chroma_collection()\n",
    "        # Fetch metadata to find unique App IDs\n",
    "        # Note: limiting to 1000 for performance; adjust if you have huge datasets\n",
    "        data = collection.get(limit=1000, include=['metadatas'])\n",
    "        metadatas = data.get('metadatas', [])\n",
    "        unique_ids = sorted(list(set(m.get('application_id') for m in metadatas if m.get('application_id'))))\n",
    "        return unique_ids\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def ui_ingest_logs(directory):\n",
    "    \"\"\"Wrapper for the ingest button.\"\"\"\n",
    "    if not directory:\n",
    "        return \"‚ö†Ô∏è Please enter a valid directory path.\"\n",
    "    try:\n",
    "        ingest_logs(directory)\n",
    "        return f\"‚úÖ Ingestion complete for: {directory}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "# ==========================================\n",
    "# 5. BUILDING THE UI\n",
    "# ==========================================\n",
    "\n",
    "with gr.Blocks(title=\"Spark Log RAG AI\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    # Header\n",
    "    gr.Markdown(\"# ‚ö° Spark Log Diagnostic AI\")\n",
    "    gr.Markdown(\"Diagnose performance issues using RAG (Retrieval Augmented Generation).\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        # --- LEFT COLUMN: Settings & Controls ---\n",
    "        with gr.Column(scale=1, variant=\"panel\"):\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Configuration\")\n",
    "            \n",
    "            # 1. Ingestion Section\n",
    "            log_dir_input = gr.Textbox(\n",
    "                label=\"Log Directory Path\", \n",
    "                placeholder=r\"C:\\path\\to\\spark-logs\",\n",
    "                value=r\"C:\\Users\\ddev\\Documents\\projects\\spark-event-logs\\test-logs\"\n",
    "            )\n",
    "            ingest_btn = gr.Button(\"üìÇ Ingest Logs\", variant=\"secondary\")\n",
    "            ingest_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "            gr.HTML(\"<hr>\") # Visual Separator\n",
    "            \n",
    "            # 2. Context Locking\n",
    "            gr.Markdown(\"### üéØ Context Scope\")\n",
    "            app_dropdown = gr.Dropdown(\n",
    "                choices=[\"All Applications\"] + get_all_app_ids(),\n",
    "                value=\"All Applications\",\n",
    "                label=\"Filter by Application ID\",\n",
    "                interactive=True\n",
    "            )\n",
    "            refresh_btn = gr.Button(\"üîÑ Refresh App List\", size=\"sm\")\n",
    "\n",
    "        # --- RIGHT COLUMN: Chat Interface ---\n",
    "        with gr.Column(scale=4):\n",
    "            chatbot = gr.Chatbot(height=600, type=\"messages\")\n",
    "            msg = gr.Textbox(\n",
    "                placeholder=\"Ask about failures, data skew, or specific stages...\",\n",
    "                container=False,\n",
    "                scale=7\n",
    "            )\n",
    "            with gr.Row():\n",
    "                clear = gr.ClearButton([msg, chatbot])\n",
    "                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    # ==========================================\n",
    "    # EVENT HANDLERS\n",
    "    # ==========================================\n",
    "\n",
    "    # 1. Ingest Button Click\n",
    "    ingest_btn.click(\n",
    "        fn=ui_ingest_logs, \n",
    "        inputs=[log_dir_input], \n",
    "        outputs=[ingest_status]\n",
    "    )\n",
    "    \n",
    "    # 2. Refresh App List Button\n",
    "    def update_dropdown():\n",
    "        ids = get_all_app_ids()\n",
    "        return gr.Dropdown(choices=[\"All Applications\"] + ids, value=\"All Applications\")\n",
    "        \n",
    "    refresh_btn.click(fn=update_dropdown, outputs=[app_dropdown])\n",
    "\n",
    "    # 3. Chat Logic (Streaming)\n",
    "    def chat_logic(user_msg, history, app_id):\n",
    "        # --- FIX: Initialize the collection here ---\n",
    "        collection = get_chroma_collection() \n",
    "        # -------------------------------------------\n",
    "\n",
    "        # 1. Handle \"Select One...\" case logic\n",
    "        target_app = app_id if app_id and app_id != \"All Applications\" else None\n",
    "\n",
    "        # 2. Append user message immediately\n",
    "        history = history + [{\"role\": \"user\", \"content\": user_msg}]\n",
    "        yield \"\", history  # Update UI to show user message first\n",
    "\n",
    "        # 3. Initialize an empty assistant response\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "\n",
    "        # 4. Consume the generator from get_gemini_response\n",
    "        for partial_response in get_gemini_response(user_msg, collection, app_id=target_app):\n",
    "            history[-1][\"content\"] = partial_response\n",
    "            yield \"\", history\n",
    "\n",
    "    # Bind Enter Key and Submit Button\n",
    "    msg.submit(chat_logic, [msg, chatbot, app_dropdown], [msg, chatbot])\n",
    "    submit_btn.click(chat_logic, [msg, chatbot, app_dropdown], [msg, chatbot])\n",
    "\n",
    "# ==========================================\n",
    "# 6. LAUNCH\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch with share=False to run locally\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False, inbrowser=True, inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797963c0-3f21-43c1-a16a-76b7f655b842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
